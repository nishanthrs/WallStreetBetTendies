{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_clean_text(text):\n",
    "    \"\"\"\n",
    "    Method that implements part of NLP pipeline of cleaning text:\n",
    "    1. Tokenization\n",
    "    2. Removing stopwords (commonly known stopwords, pronouns, keywords with little info, words less than 2 chars)\n",
    "    3. Lemmatization\n",
    "\n",
    "    Args: \n",
    "    text to be tokenized and cleaned\n",
    "\n",
    "    Returns: list of tokens (spacy doc) and list of lemmatized words (list of strings)\n",
    "    \"\"\"\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    post_doc = nlp(text)\n",
    "    \n",
    "    # Clean spacy doc and structure as 2D numpy array of spacy tokens\n",
    "    cleaned_post_doc = []\n",
    "    for sent in post_doc.sents:\n",
    "        cleaned_sent = [\n",
    "            token.lemma_.lower().strip() for token in sent \n",
    "            if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-'\n",
    "        ]\n",
    "        if len(cleaned_sent) > 0:\n",
    "            cleaned_post_doc.append(cleaned_sent)\n",
    "    \n",
    "    return cleaned_post_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_text(text):\n",
    "    return [token for sent in text for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_keywords(post):\n",
    "    cleaned_post = tokenize_and_clean_text(post)\n",
    "    print(cleaned_post)\n",
    "    flattened_cleaned_post = flatten_text(cleaned_post)\n",
    "    print(flattened_cleaned_post)\n",
    "    word_freq = Counter(flattened_cleaned_post)\n",
    "    print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB is hiring a new chief product officer He's supposed to be a veteran in the internet business. the internet is great\n",
      "[['fb', 'hire', 'new', 'chief', 'product', 'officer'], ['suppose', 'veteran', 'internet', 'business'], ['internet', 'great']]\n",
      "['fb', 'hire', 'new', 'chief', 'product', 'officer', 'suppose', 'veteran', 'internet', 'business', 'internet', 'great']\n",
      "<class 'collections.Counter'>\n"
     ]
    }
   ],
   "source": [
    "comment = 'FB is hiring a new chief product officer He\\'s supposed to be a veteran in the internet business. the internet is great'\n",
    "get_most_common_keywords(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True\n",
    "    )\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    return ' '.join(flatten_text(tokenize_and_clean_text(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_sentence'] = train['sentence'].apply(\n",
    "    lambda sentence: clean_sentence(sentence)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['cleaned_sentence'] = test['sentence'].apply(\n",
    "    lambda sentence: clean_sentence(sentence)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Webb Peoples meets Paul Anderson...if it already sounds weird to you, then you are right, because it is.<br /><br />Peoples is known for his scripts with moral implications of what is right and wrong, the value of life, etc... He covered these issues in Bladerunner, Unforgiven, and pretty much in all of his screenplays there is something along those lines.<br /><br />Paul Anderson's first successful movie was a violent thriller. Not surprisingly so have all of his other movies! And here is a violent thriller with moral implications!<br /><br />Peoples' script is quite apparent in the first half of the movie. Soldiers trained from birth, taught to kill, and never had a normal life. They are replaced by better, genetically engineered soldiers and Todd, one of the original soldiers, is left on a planet and left for dead. There he must cope with a group of refugees, some want him to stay others hate him and there is an interesting drama here. BUT THEN...<br /><br />...The bullets start to fly as the new soldiers move onto the planet for a military exercise and try to kill all the people. Big, violent, loud action ensues and Peoples' script turns into an Anderson action-fest. It is hard to believe that the script was originally written that way, but the end product is better then I expected. Entertaining, somewhat, though admittedly not very, thought-provoking, and exciting once the action starts. 7/10<br /><br />Rated R: a lot of violence\n",
      "david webb peoples meet paul anderson sound weird right is.<br /><br />peoples know script moral implication right wrong value life etc cover issue bladerunner unforgiven pretty screenplay lines.<br /><br />paul anderson successful movie violent thriller surprisingly movie violent thriller moral implications!<br /><br />peoples script apparent half movie soldier train birth teach kill normal life replace well genetically engineer soldier todd original soldier leave planet leave dead cope group refugee want stay hate interesting drama <br /><br /> bullet start fly new soldier planet military exercise try kill people big violent loud action ensue people script turn anderson action f hard believe script originally write way end product well expect entertaining somewhat admittedly thought provoking exciting action start 7/10 < br /><br />rated r lot violence\n"
     ]
    }
   ],
   "source": [
    "print(train['sentence'].iloc[0])\n",
    "print(train['cleaned_sentence'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'cleaned_sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def run_textblob_sentiment(sentences, sentiment_labels):\n",
    "    sentiments = []\n",
    "    for sentence in sentences:\n",
    "        if TextBlob(sentence).polarity > 0:\n",
    "            sentiments.append(1)\n",
    "        else:\n",
    "            sentiments.append(0)\n",
    "    \n",
    "    print('Model accuracy: ', accuracy_score(sentiments, sentiment_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def run_random_forest_classification(train_features, test_features, y_train, y_test):\n",
    "    max_features = int(.8 * train_features.shape[1])\n",
    "    rf = RandomForestClassifier(max_depth=30, n_estimators=500, max_features=max_features, n_jobs=-1)\n",
    "    \n",
    "    rf.fit(train_features, y_train)\n",
    "    train_pred = rf.predict(train_features)\n",
    "    train_model_accuracy = accuracy_score(train_pred, y_train)\n",
    "    \n",
    "    test_pred = rf.predict(test_features)\n",
    "    print('Dist of predictions: ', np.unique(test_pred, return_counts=True))\n",
    "    print('Dist of actual deltas: ', np.unique(y_test, return_counts=True))\n",
    "    test_model_accuracy = accuracy_score(test_pred, y_test)\n",
    "    print('Test model accuracy: ', test_model_accuracy)\n",
    "    \n",
    "    filename = 'tendies/finalized_sentiment_model.joblib'\n",
    "    joblib.dump(rf, filename)\n",
    "\n",
    "    # load the model from disk\n",
    "    '''\n",
    "    loaded_model = joblib.load(filename)\n",
    "    result = loaded_model.predict(X_test, Y_test)\n",
    "    print(result)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "final_train = train[DATA_COLUMN].append(test[DATA_COLUMN])\n",
    "X = vectorizer.fit_transform(final_train)\n",
    "X_labels = train[LABEL_COLUMN].append(test[LABEL_COLUMN])\n",
    "test_size = 0.2\n",
    "\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    X, X_labels, test_size=test_size, stratify=X_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 91597)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist of predictions:  (array([0, 1]), array([4693, 5307]))\n",
      "Dist of actual deltas:  (array([0, 1]), array([5000, 5000]))\n",
      "Test model accuracy:  0.7945\n"
     ]
    }
   ],
   "source": [
    "run_random_forest_classification(\n",
    "    train_features, val_features, train_labels, val_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy:  0.71715\n"
     ]
    }
   ],
   "source": [
    "run_textblob_sentiment(train[DATA_COLUMN].iloc[0:20000], train[LABEL_COLUMN][0:20000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
